{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a neural network, the input matrix is often represented as a **row vector**, where each element of the vector represents a feature. However, in order to perform matrix multiplication with the weights matrix, the input matrix must be transposed to a **column vector**.\n",
    "\n",
    "The reason for transposing the input matrix is related to the way **matrix multiplication** works. When two matrices are multiplied, the number of columns in the first matrix must match the number of rows in the second matrix. In a neural network, **the input matrix represents a single training example, with each feature represented as a row vector.** The weights matrix represents the connections between the input and hidden layers of the network, with each row of the weights matrix corresponding to a single neuron in the hidden layer.\n",
    "\n",
    "To perform matrix multiplication between the input matrix and the weights matrix, the input matrix **needs to be transposed to a column vector**, *so that* the **number of columns matches the number of rows in the weights matrix**. The resulting product of the transpose of the input matrix and the weights matrix is a column vector, which represents the output of the hidden layer.\n",
    "\n",
    "In summary, the input matrix is transposed in a neural network before it is multiplied to the weights matrix in order to perform matrix multiplication that represents the connections between the input and hidden layers of the network. The resulting product is a column vector, which represents the output of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,  25.,  50.,  75., 100.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "# Here we initialize a row vector of shape (1,5) , in which each element represents a feature\n",
    "X = np.array([np.linspace(0,100,5)])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.],\n",
       "       [ 25.],\n",
       "       [ 50.],\n",
       "       [ 75.],\n",
       "       [100.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We transpose our feature vector to a column vector \n",
    "# to prepare for matrix multiplication\n",
    "X = X.T\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our input layer will have 5 neurons in it, arranged as a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2\n",
    "# We have an array of 3 examples, 2 elements per example\n",
    "X_2 = np.array([[3,5], [5,1], [10,2]], dtype=float)\n",
    "X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  5., 10.],\n",
       "       [ 5.,  1.,  2.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2transposed = X_2.T\n",
    "X_2transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2transposed.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if we have 2 input features per example, our input layer will have 2 neurons. \n",
    "\n",
    "- **3**, **5**, and **10** will go to the **first** neuron\n",
    "- **5**,**1,** and **10** will go to the **second** neuron."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**\n",
    "\n",
    "Our data is composed of two input variables and one output variable. Our two input variables are **hours of sleep** and **hours of study**, while our output variable is **score on test**.\n",
    "\n",
    "We initially have three examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  5.]\n",
      " [ 5.  1.]\n",
      " [10.  2.]]\n",
      "[[75.]\n",
      " [82.]\n",
      " [93.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[3,5], [5,1], [10,2]], dtype=float)\n",
    "y = np.array([[75],[82],[93]], dtype=float)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs are now arranged per example, with each example containing 2 features. In application, we do not need to specifically transpose our input data if it is laid out per example. Our neural network that we will build will be the one to take the first element of each example and pass it as an input to the first neuron of the first layer, while the second element will be passed as an input to the second neuron of the first layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling our data through Min-max Normalization**\n",
    "\n",
    "We need to scale our data, because our input is in the range of single values, while our output is in the range of two digit values. This will help our neural network learn better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.44444444],\n",
       "       [0.44444444, 0.        ],\n",
       "       [1.        , 0.11111111]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.38888889],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (y - y.min()) / (y.max() - y.min())\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our input and output features are of the same scale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Neural Network class in Python**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to Remember:**\n",
    "\n",
    "1. Our input matrix is in the shape of (number of examples, number of features).\n",
    "2. Our weights matrix is in the shape of (number of neurons of the previous layer, number of neurons of the next layer).\n",
    "3. Our hidden layer is in the shape of (number of examples, number of neurons in the hidden layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self):\n",
    "        # Define hyperparameters\n",
    "        self.input_layer_size = 2\n",
    "        self.output_layer_size = 1\n",
    "        self.hidden_layer_size = 3\n",
    "\n",
    "        # Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer_size)\n",
    "        self.W2 = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
    "        # self.bias = np.random.randn(1, dtype=float)\n",
    "    def forward(self, X):\n",
    "        # Propagate inputs through network\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "\n",
    "        return yHat\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        #applies sigmoid function element-wise, and returns the same shape as the input\n",
    "        return 1 / 1 + np.exp(-z)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Propagation Notation of our Neural Network**\n",
    "\n",
    "$$ z^{(2)} = X^{T} \\cdot W^{(1)} $$\n",
    "\n",
    "$$ a^{(2)} = f(z^{(2)})  $$\n",
    "\n",
    "$$ z^{(3)} = a^{(3)} \\cdot W^{(2)} $$\n",
    "\n",
    "$$ \\hat{y} = f(z^{(3)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.44444444],\n",
       "       [0.44444444, 0.        ],\n",
       "       [1.        , 0.11111111]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()\n",
    "yhat = NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.38427661],\n",
       "       [52.29225278],\n",
       "       [77.57190155]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.38888889],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions are still far from our true value since we have not trained our network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training our Network**\n",
    "\n",
    "It means minimizing a cost function, or the difference between our predictions and the true value.\n",
    "\n",
    "Our cost is a function of two things: **our examples, and the weights of our synapses**. We don't have much control on our data, but we can minimize the cost by changing the weights.\n",
    "\n",
    "$$ J(cost) = \\sum \\frac{1}{2} (y - \\hat{y})^{2} $$\n",
    "\n",
    "For our first weights matrix, `W1`, it has 9 weights in it. Say, we want to know how fast can we test some values to minimize the cost for **one** weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "weights_to_try = np.linspace(-5, 5, 1000)\n",
    "costs = np.zeros(1000)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    NN.W1[0,0] = weights_to_try[i]\n",
    "    yhat = NN.forward(X)\n",
    "    costs[i] = 0.5 * sum((y-yhat)**2)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03986549377441406"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_elapsed = end - start\n",
    "time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(weights_to_try, costs)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Weight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we want to know two weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "weights_to_try = np.linspace(-5, 5, 1000)\n",
    "costs = np.zeros((1000,1000))\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        NN.W1[0,0] = weights_to_try[i]\n",
    "        NN.W1[0,1] = weights_to_try[i]\n",
    "        yhat = NN.forward(X)\n",
    "        costs[i] = 0.5 * sum((y-yhat)**2)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.67851948738098"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_elapsed = end - start\n",
    "time_elapsed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we continue to add dimensions, searching through all 9 weights will take more than a lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268391679350583.5 years.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{0.04*(1000**(9-1))/(3600*24*365)} years.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z^{(2)} = X^{T} \\cdot W^{(1)} $$\n",
    "\n",
    "$$ a^{(2)} = f(z^{(2)})  $$\n",
    "\n",
    "$$ z^{(3)} = a^{(3)} \\cdot W^{(2)} $$\n",
    "\n",
    "$$ \\hat{y} = f(z^{(3)}) $$\n",
    "\n",
    "$$ J(cost) = \\sum \\frac{1}{2} (y - \\hat{y})^{2} $$\n",
    "\n",
    "$$ J = \\sum \\frac{1}{2} (y -f(f(X^{T} \\cdot W^{(1)}) W^{(2)}))^{2} $$\n",
    "\n",
    "Now, we have one big equation that uniquely determines our cost **J** from X, y, W1 and W2, we can use calculus to find exactly what we're looking for.\n",
    "\n",
    "We want to know which way is downhill, that is what is the **rate of change of J with respect to W**, or $\\frac {dJ}{dW}$.\n",
    "\n",
    "And in this case, since we are only considering one weight at a time, this is a partial derivative.\n",
    "\n",
    "$$\\frac {\\partial J}{\\partial W} $$\n",
    "\n",
    "We can derive an expression for $\\frac {\\partial J}{\\partial W} $ that will give us the rate of change of J with respect to W for any value of W.\n",
    "\n",
    "If $\\frac {\\partial J}{\\partial W} = + $, then the cost function is going uphill.\n",
    "\n",
    "If $\\frac {\\partial J}{\\partial W} = - $, then the cost function is going downhill.\n",
    "\n",
    "Now we can speed things up. Since we know which direction the cost decreases, we can save the time we would have used searching in the wrong direction. We can save more computational time by iteratively taking steps downhill, and stopping when the cost gets to **0**.\n",
    "\n",
    "This method is called **Gradient Descent**. Although it may not look impressive in one dimension, it can incredibly speed up things in the higher dimension. Gradient descent allows us to find needles in very, very, very large haystack.\n",
    "\n",
    "But there is a restriction. What if the cost function does not always go in the same direction. What if it goes up, then back down. The mathematical name for this path is called **non-convex**. And it could get gradient descent stuck in a **local minimum**, instead of the **global minimum**.\n",
    "\n",
    "One of the reasons that we choose our cost function to be the sum of square errors is to exploit the nature of quadratic equations. We know that the graph of $y=x^{2}$ is a nice convex parabola, and it turns out to be also true in higher dimensions (but not always).\n",
    "\n",
    "Another piece of the puzzle is, depending on how we use our data, it might not matter if our function is convex or not.\n",
    "\n",
    "If we use our examples one at a time, instead of all at once, sometimes it would not matter if our function is convex, we would still find a **good solution**. This is called **stochastic gradient descent**. So maybe we should not be afraid of non-convex loss function. ~ Yann Lecun (Who is afraid of non-convex loss functions?)\n",
    "\n",
    "For now we will use our examples all at once, where we find the cost from each example and then add them up, before we move to the direction of lower cost. This is called **batch gradient descent**, and the way we set up our cost function will make things nice and convex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dem-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
